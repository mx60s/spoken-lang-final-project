{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88bb930-a1ce-4ace-abe4-1bfc657262fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the task prediction setup for HEAR\n",
    "# https://github.com/hearbenchmark/hear-eval-kit/blob/main/heareval/predictions/task_predictions.py\n",
    "\n",
    "# HEAR is trying to evaluate audio FEATURES for these tasks\n",
    "# so it fits it with some basic decoder fully-connected net to do classification\n",
    "# we're gonna need to modify that for this in conjunction with how you should \n",
    "# modify the huggingface stuff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327665e2-d1bc-4803-987b-50d8e19882d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apparently python3.9 is not officially supported for hear eval due to pip3 install issues\n",
    "\n",
    "! pip3 install heareval\n",
    "! pip3 install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d177ef3f-9e19-48ed-a98a-2113cc1d9f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This framework is currently just lifted from this tutorial: \n",
    "# https://towardsdatascience.com/fine-tuning-hubert-for-emotion-recognition-in-custom-audio-data-using-huggingface-c2d516b41cd8\n",
    "# They use Weights&Biases framework for saving model progress\n",
    "# but idk what that is honestly so leaving it alone for now\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import librosa\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0bbcd7-dc80-4b44-a3e9-f51f5414e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, load_dataset, load_metric\n",
    "from transformers import (\n",
    "    HubertForSequenceClassification,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    ")\n",
    "from utils import collator #?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d777f0fc-4605-4217-a802-666400c2f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))\n",
    "NUM_LABELS = 6\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)s: %(message)s\", level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd94e90-db8d-45f9-80d4-2ef3d8349156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS THE DATASET TO THE FORMAT EXPECTED BY THE MODEL FOR TRAINING\n",
    "PreTrainedFeatureExtractor = \"SequenceFeatureExtractor\"  # noqa: F821\n",
    "\n",
    "INPUT_FIELD = \"input_values\"\n",
    "LABEL_FIELD = \"labels\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4d07d7-fd4e-4eb7-b9da-564836f13745",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"facebook/hubert-base-ls960\"\n",
    "MODELS_DIR = os.path.join(PROJECT_ROOT, \"models\")\n",
    "\n",
    "extractor_path = (\n",
    "    model_id\n",
    "    if len(os.listdir(MODELS_DIR)) == 0\n",
    "    else os.path.join(MODELS_DIR, \"feature_extractor\")\n",
    ")\n",
    "model_path = (\n",
    "    model_id\n",
    "    if len(os.listdir(MODELS_DIR)) == 0\n",
    "    else os.path.join(MODELS_DIR, \"pretrained_model\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d6283-0e25-4723-a309-656b79c70e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(extractor_path)\n",
    "\n",
    "config = PretrainedConfig.from_pretrained(model_path, num_labels=NUM_LABELS)\n",
    "hubert_model = HubertForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    config=config,  # because we need to update num_labels as per our dataset\n",
    "    ignore_mismatched_sizes=True,  # to avoid classifier size mismatch from from_pretrained.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca187be-54d8-4775-b612-b918ee2fa2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREEZE LAYERS\n",
    "\n",
    "# freeze all layers to begin with\n",
    "for param in hubert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "layers_freeze_num = 2\n",
    "n_layers = (\n",
    "    4 + layers_freeze_num * 16\n",
    ")  # 4 refers to projector and classifier's weights and biases.\n",
    "for name, param in list(hubert_model.named_parameters())[-n_layers:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "# # freeze model weights for all layers except projector and classifier\n",
    "# for name, param in hubert_model.named_parameters():\n",
    "#     if any(ext in name for ext in [\"projector\", \"classifier\"]):\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb07af23-89ef-462a-9b00-25a9ef929f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = {\n",
    "    \"OUTPUT_DIR\": \"results\",\n",
    "    \"TRAIN_EPOCHS\": 5,\n",
    "    \"TRAIN_BATCH_SIZE\": 32,\n",
    "    \"EVAL_BATCH_SIZE\": 32,\n",
    "    \"GRADIENT_ACCUMULATION_STEPS\": 4,\n",
    "    \"WARMUP_STEPS\": 500,\n",
    "    \"DECAY\": 0.01,\n",
    "    \"LOGGING_STEPS\": 10,\n",
    "    \"MODEL_DIR\": \"models/audio-model\",\n",
    "    \"LR\": 1e-3,\n",
    "}\n",
    "\n",
    "\n",
    "dataset_config = {\n",
    "    \"LOADING_SCRIPT_FILES\": os.path.join(PROJECT_ROOT, \"src/data/crema.py\"),\n",
    "    \"CONFIG_NAME\": \"clean\",\n",
    "    \"DATA_DIR\": os.path.join(PROJECT_ROOT, \"data/archive.zip\"),\n",
    "    \"CACHE_DIR\": os.path.join(PROJECT_ROOT, \"cache_crema\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daeebd3-31aa-414d-aa9b-a1c03a210010",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\n",
    "    dataset_config[\"LOADING_SCRIPT_FILES\"],\n",
    "    dataset_config[\"CONFIG_NAME\"],\n",
    "    cache_dir=dataset_config[\"CACHE_DIR\"],\n",
    "    data_dir=dataset_config[\"DATA_DIR\"],\n",
    ")\n",
    "\n",
    "\n",
    "# CONVERING RAW AUDIO TO ARRAYS\n",
    "ds = ds.map(\n",
    "    lambda x: {\"array\": librosa.load(x[\"file\"], sr=16000, mono=False)[0]},\n",
    "    num_proc=2,\n",
    ")\n",
    "\n",
    "\n",
    "# LABEL TO ID\n",
    "ds = ds.class_encode_column(\"label\")\n",
    "\n",
    "# APPLY THE DATA PREP USING FEATURE EXTRACTOR TO ALL EXAMPLES\n",
    "ds = ds.map(\n",
    "    prepare_dataset,\n",
    "    fn_kwargs={\"feature_extractor\": feature_extractor},\n",
    "    # num_proc=4,\n",
    ")\n",
    "logging.info(\"Finished extracting features from audio arrays.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd3e0c-472d-49b0-acc1-d5e5b5ef77dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL TO ID\n",
    "ds = ds.class_encode_column(\"label\")\n",
    "\n",
    "\n",
    "# ds[\"train\"] = ds[\"train\"].select(range(2500))\n",
    "wandb.log({\"dataset_size\": len(ds[\"train\"])})\n",
    "\n",
    "\n",
    "# APPLY THE DATA PREP USING FEATURE EXTRACTOR TO ALL EXAMPLES\n",
    "ds = ds.map(\n",
    "    prepare_dataset,\n",
    "    fn_kwargs={\"feature_extractor\": feature_extractor},\n",
    "    # num_proc=4,\n",
    ")\n",
    "logging.info(\"Finished extracting features from audio arrays.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34ac7f6-c25c-4471-9233-bdce82f63627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE DATA COLLATOR - TO PAD TRAINING BATCHES DYNAMICALLY\n",
    "data_collator = collator.DataCollatorCTCWithPadding(\n",
    "    processor=feature_extractor, padding=True\n",
    ")\n",
    "\n",
    "\n",
    "# Fine-Tuning with Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(\n",
    "        PROJECT_ROOT, trainer_config[\"OUTPUT_DIR\"]\n",
    "    ),  # output directory\n",
    "    gradient_accumulation_steps=trainer_config[\n",
    "        \"GRADIENT_ACCUMULATION_STEPS\"\n",
    "    ],  # accumulate the gradients before running optimization step\n",
    "    num_train_epochs=trainer_config[\"TRAIN_EPOCHS\"],  # total number of training epochs\n",
    "    per_device_train_batch_size=trainer_config[\n",
    "        \"TRAIN_BATCH_SIZE\"\n",
    "    ],  # batch size per device during training\n",
    "    per_device_eval_batch_size=trainer_config[\n",
    "        \"EVAL_BATCH_SIZE\"\n",
    "    ],  # batch size for evaluation\n",
    "    warmup_steps=trainer_config[\n",
    "        \"WARMUP_STEPS\"\n",
    "    ],  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=trainer_config[\"DECAY\"],  # strength of weight decay\n",
    "    logging_steps=trainer_config[\"LOGGING_STEPS\"],\n",
    "    evaluation_strategy=\"epoch\",  # report metric at end of each epoch\n",
    "    report_to=\"wandb\",  # enable logging to W&B\n",
    "    learning_rate=trainer_config[\"LR\"],  # default = 5e-5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d42d2-1b26-44d5-a328-e93e92af0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TRAINING\n",
    "trainer = Trainer(\n",
    "    model=hubert_model,  # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,  # training arguments, defined above\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=ds[\"train\"],  # training dataset\n",
    "    eval_dataset=ds[\"val\"],  # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e47e3d-ff9b-4754-a651-44b941be0415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO RESUME TRAINING FROM CHECKPOINT\n",
    "# trainer.train(\"results/checkpoint-2000\")\n",
    "\n",
    "# VALIDATION SET RESULTS\n",
    "logging.info(\"Eval Set Result: {}\".format(trainer.evaluate()))\n",
    "\n",
    "# TEST RESULTS\n",
    "test_results = trainer.predict(ds[\"test\"])\n",
    "logging.info(\"Test Set Result: {}\".format(test_results.metrics))\n",
    "\n",
    "trainer.save_model(os.path.join(PROJECT_ROOT, trainer_config[\"MODEL_DIR\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
